The project focuses on lip reading from audioless video using deep learning techniques. The system processes video frames, converts them into grayscale, and extracts relevant mouth movement data. It utilizes a vocabulary-based character mapping system to interpret speech from visual cues. A deep neural network, including Conv3D, LSTMs, and Bidirectional layers, is trained to predict spoken words from silent videos. The project includes a data pipeline for loading and aligning videos with text, model training with CTC loss, and evaluation on test samples. The final model achieves accurate predictions, demonstrating its potential for silent communication and accessibility applications.
